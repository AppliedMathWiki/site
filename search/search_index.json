{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>"},{"location":"#optwiki","title":"OptWiki","text":"<p> Collective Wisdom for Modeling and Optimizing</p> <p></p> <p>See the tags page for lists of all tagged articles.</p> <p>Random Algorithm </p> <p>Random Model </p> <p></p>"},{"location":"#how-it-works","title":"How it Works","text":"<p>Search for algorithms / models in search bar above.</p> <p>Anyone can submit edits / additions via Github.<sup>1</sup> </p> <p>Content discussion is facilitated via Github issues.</p> <p></p> <p>Contact Us</p> <p></p> <p></p> <ol> <li> <p>Note the pencil button on each page to make an edit.\u00a0\u21a9</p> </li> </ol>"},{"location":"tags/","title":"Tags","text":"<p>Below is a list of all tags on OptWiki.</p>"},{"location":"tags/#algorithm","title":"algorithm","text":"<ul> <li>Franke-Wolfe</li> </ul>"},{"location":"tags/#conditional-gradient","title":"conditional-gradient","text":"<ul> <li>Franke-Wolfe</li> </ul>"},{"location":"tags/#convex","title":"convex","text":"<ul> <li>Lasso</li> </ul>"},{"location":"tags/#franke-wolfe","title":"franke-wolfe","text":"<ul> <li>Franke-Wolfe</li> </ul>"},{"location":"tags/#model","title":"model","text":"<ul> <li>Lasso</li> </ul>"},{"location":"algorithm/franke-wolfe/","title":"Franke-Wolfe","text":"","tags":["algorithm","franke-wolfe","conditional-gradient"]},{"location":"algorithm/franke-wolfe/#definition","title":"Definition","text":"<p>For a constraint set \\(\\mathcal{C} \\subseteq \\mathbb{H}\\)<sup>1</sup>  and a function<sup>2</sup> \\(\\ \\mathsf{f\\colon \\mathbb{H} \\rightarrow \\overline{\\mathbb{R}}}\\) that is differentiable at \\(\\mathsf{x^k \\in \\mathbb{H}}\\), the Franke-Wolfe algorithm is an iterative scheme with update \\(\\mathsf{x^{k+1}}\\) is defined by </p> \\[     \\mathsf{x^{k+1} \\triangleq \\underset{x\\in \\mathcal{C}}{argmin} \\left&lt; \\nabla f(x^k),\\  x \\right&gt; }. \\]","tags":["algorithm","franke-wolfe","conditional-gradient"]},{"location":"algorithm/franke-wolfe/#overview","title":"Overview","text":"<p>The Frank\u2013Wolfe algorithm, also called \"conditional gradient\", is an iterative first-order algorithm for solving constrained convex problems. The method was proposed by Marguerite Frank and Philip Wolfe in 1956.<sup>3</sup> Each update minimimizes the linearization of \\(\\mathsf{f}\\) about \\(\\mathsf{x^k}\\) (i.e. \\(\\mathsf{f(x^k)+\\left&lt;\\nabla f(x^k),\\ x - x^k\\right&gt;})\\) over the domain \\(\\mathcal{C}\\). </p> <p>The objective in the update is equivalent to  Maintains feasibility. Is equivalent to minimizing linearizations.</p>","tags":["algorithm","franke-wolfe","conditional-gradient"]},{"location":"algorithm/franke-wolfe/#properties","title":"Properties","text":"<p>Convergence Theorem<sup>4</sup> </p> <p>If \\(\\mathsf{f}\\) is convex and Lipschitz differentiable over a convex and compact set \\(\\mathcal{C}\\), then there is a constant \\(\\mathsf{C&gt; 0}\\) such that</p> \\[ \\mathsf{f(x^k) \\leq f(x^\\star) + \\dfrac{C}{k+2},} \\] <p>where \\(\\mathsf{x^\\star}\\) is a minimizer of \\(\\mathsf{f}\\) over \\(\\mathcal{C}\\).</p> <p>Zigzag Behavior<sup>5</sup> </p> <p>Oscillations can occur...</p> <p>Invariant to Coordinate System<sup>6</sup></p> <p>Can rescale or re-jigger coordinates without any change in outcomes.</p> <p>Projection-Free</p> <p>No projections needed if \\(\\mathsf{x^1 \\in \\mathcal{C}.}\\)</p>","tags":["algorithm","franke-wolfe","conditional-gradient"]},{"location":"algorithm/franke-wolfe/#code","title":"Code","text":"Python (Linear Constraint)Python (Ellipsoid Constraint) <pre><code># Constraint: C = { x : A * x = b }\nA = np.randn(100, 5)\nb = np.randn(5, 1)\n\ndef get_fw_solution(gradf, A, b):\n    \"\"\" Compute Franke-Wolfe solution\n\n        args:\n          x: ...\n    \"\"\"\n    x         = init\n    num_iters = 100\n\n    for _ in range(num_iters):\n        x = x - grad_f(x)\n\n    return x\n</code></pre> <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\n  std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre>","tags":["algorithm","franke-wolfe","conditional-gradient"]},{"location":"algorithm/franke-wolfe/#applications","title":"Applications","text":"<ul> <li>Statistics... </li> <li>Compressed sensing...</li> </ul>","tags":["algorithm","franke-wolfe","conditional-gradient"]},{"location":"algorithm/franke-wolfe/#see-also","title":"See Also","text":"<ul> <li>Wikipedia: Frank-Wolfe algorithm</li> <li>variations of franke-wolfe.</li> </ul> <p>{% block footer %}</p> <ul> <li> Contributors <ul>       {%- for user in committers -%}       <li></li>       {%- endfor -%}     </ul> </li> </ul> <p>{% endblock %}</p> <ol> <li> <p>We let \\(\\mathbb{H}\\) be a real-valued finite dimensional Hilbert space (e.g \\(\\ \\mathbb{H} = \\mathbb{R}^{\\mathsf{n}}\\)).\u00a0\u21a9</p> </li> <li> <p>Here \\(\\overline{\\mathbb{R}}\\triangleq \\mathbb{R} \\cup \\infty.\\) \u21a9</p> </li> <li> <p>Frank, F., Wolfe, P. An algorithm for quadratic programming. Naval Research Logistics Quarterly. 1956.\u00a0\u21a9</p> </li> <li> <p>Jaggi, M. Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization. ICML. 2013.\u00a0\u21a9</p> </li> <li> <p>Insert citations to Jaggi paper.\u00a0\u21a9</p> </li> <li> <p>Use Taylor's theorem or something...\u00a0\u21a9</p> </li> </ol>","tags":["algorithm","franke-wolfe","conditional-gradient"]},{"location":"model/lasso/","title":"Lasso","text":"","tags":["model","convex"]},{"location":"model/lasso/#model","title":"Model","text":"<p>For a matrix \\(\\mathsf{A \\in \\mathbb{R}^{m\\times n}}\\), a vector \\(\\mathsf{b \\in \\mathbb{R}^m}\\), and a scalar \\(\\mathsf{\\lambda &gt; 0}\\), the model is</p> \\[     \\mathsf{\\underset{x \\in \\mathbb{R}^n }{min} \\ |Ax - b|_2^2 + \\lambda |x|_1. } \\]","tags":["model","convex"]},{"location":"model/lasso/#overview","title":"Overview","text":"<p>The Least Absolute Shrinkage and Selection Operator (Lasso) function is commonly used in statistics and machine learning for variable selection and regularization. Introduced by Tibshirani<sup>1</sup>, it is primarily used in linear regression models, but its principles can be extended to other models as well. The key idea of the Lasso function is to add a penalty term to least squares regression that is the sum of absolute values of the coefficients.</p> <p></p> <p> Property Convex \u2705 Strongly Convex \ud83d\udfe1 Unbiased Estimator<sup>2</sup> \u2705 Feasible Estimator \ud83d\udfe1 <p></p> <p></p>","tags":["model","convex"]},{"location":"model/lasso/#code","title":"Code","text":"CVXPYSCIPGurobiCPLEX <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\n  std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre> <pre><code>#include &lt;stdio.h&gt;\n\nint main(void) {\n  printf(\"Hello world!\\n\");\n  return 0;\n}\n</code></pre> <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\n  std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre> <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\n  std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre>","tags":["model","convex"]},{"location":"model/lasso/#applications","title":"Applications","text":"<ul> <li>Statistics... </li> <li>Compressed sensing...</li> </ul>","tags":["model","convex"]},{"location":"model/lasso/#see-also","title":"See Also","text":"<ul> <li>Wikipedia - Lasso (statistics)</li> </ul> <ol> <li> <p>Tibshirani. \"Regression shrinkage and selection via the lasso.\" Journal of the Royal Statistical Society Series B: Statistical Methodology. 1996\u00a0\u21a9</p> </li> <li> <p>Fan, Li. \"Variable selection via nonconcave penalized likelihood and its oracle properties.\" Journal of the American statistical Association. 2001.\u00a0\u21a9</p> </li> </ol>","tags":["model","convex"]},{"location":"tags/","title":"Tags","text":"<p>Below is a list of all tags on OptWiki.</p>"},{"location":"tags/#algorithm","title":"algorithm","text":"<ul> <li>Franke-Wolfe</li> </ul>"},{"location":"tags/#conditional-gradient","title":"conditional-gradient","text":"<ul> <li>Franke-Wolfe</li> </ul>"},{"location":"tags/#convex","title":"convex","text":"<ul> <li>Lasso</li> </ul>"},{"location":"tags/#franke-wolfe","title":"franke-wolfe","text":"<ul> <li>Franke-Wolfe</li> </ul>"},{"location":"tags/#model","title":"model","text":"<ul> <li>Lasso</li> </ul>"}]}