{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> OptWiki <p> Collective Wisdom for Modeling and Optimizing</p> <p></p> <p>Random Algorithm </p> <p>Random Model </p> <p>See the tags page for lists of all tagged articles.</p> <p></p>"},{"location":"#optwiki-mission","title":"OptWiki Mission","text":"<p>To make new and established optimization tools and techniques widely accessible.</p> <p></p>"},{"location":"#how-it-works","title":"How it Works","text":"<p>Users: Search for algorithms / models in search bar above.</p> <p>Contributors: Submit edits / additions via Github.<sup>1</sup> </p> <p>OptWiki strives for commons-based peer production<sup>2</sup> whereby we encourage anyone with interest to contribute.</p> <p></p> <p>Contact Us</p> <p></p> <p></p> <ol> <li> <p>Note the pencil button on each page to make an edit.\u00a0\u21a9</p> </li> <li> <p>This is based on Benkler's essay Coase's Penguin, Or, Linux and 'The Nature of the Firm' about collaboratively producing online.\u00a0\u21a9</p> </li> </ol>"},{"location":"tags/","title":"Tags","text":"<p>Below is a list of all tags on OptWiki.</p>"},{"location":"tags/#algorithm","title":"algorithm","text":"<ul> <li>Frank-Wolfe</li> </ul>"},{"location":"tags/#conditional-gradient","title":"conditional-gradient","text":"<ul> <li>Frank-Wolfe</li> </ul>"},{"location":"tags/#convex","title":"convex","text":"<ul> <li>Lasso</li> </ul>"},{"location":"tags/#first-order","title":"first-order","text":"<ul> <li>Frank-Wolfe</li> </ul>"},{"location":"tags/#frank-wolfe","title":"frank-wolfe","text":"<ul> <li>Frank-Wolfe</li> </ul>"},{"location":"tags/#model","title":"model","text":"<ul> <li>Lasso</li> </ul>"},{"location":"tags/#projection-free","title":"projection-free","text":"<ul> <li>Frank-Wolfe</li> </ul>"},{"location":"algorithm/frank-wolfe/","title":"Frank-Wolfe","text":"<p>Note from Editors</p> <p>This article has been flagged as incomplete for the following:</p> <ul> <li> Incomplete / poorly formatted list of properties</li> <li> Inadequate references to applications (Section: Applications)    </li> <li> Inadequate references to related works (Section: See Also)</li> </ul>","tags":["algorithm","frank-wolfe","conditional-gradient","first-order","projection-free"]},{"location":"algorithm/frank-wolfe/#definition","title":"Definition","text":"<p>For a constraint set \\(\\mathcal{C} \\subseteq \\mathbb{H}\\)<sup>1</sup>  and a function<sup>2</sup> \\(\\ \\mathsf{f\\colon \\mathbb{H} \\rightarrow \\overline{\\mathbb{R}}}\\) that is differentiable over \\(\\mathcal{C}\\), consider the constrained minimization problem</p> \\[     \\mathsf{\\underset{x \\in \\mathcal{C}}{min} \\ f(x).} \\] <p>Frank-Wolfe is an iterative scheme for solving this problem with update \\(\\mathsf{x^{k+1}}\\) defined by </p> \\[\\begin{aligned}     \\mathsf{s^{k}} &amp; \\mathsf{\\triangleq \\underset{x\\in \\mathcal{C}}{argmin} \\ \\left&lt; \\nabla f(x^k),\\  x \\right&gt;, } \\\\     \\mathsf{x^{k+1}} &amp; \\mathsf{\\triangleq x^k + \\alpha_k \\left( s^k - x^k \\right),} \\end{aligned}\\] <p>with a step size rule typically given by either</p> \\[ \\mathsf{\\alpha_k = \\dfrac{2}{k+2} \\quad {or} \\quad \\alpha_k = \\underset{\\alpha\\in[0,1]}{argmin} \\ f\\left(x^k + \\alpha (s^k - x^k)\\right).} \\]","tags":["algorithm","frank-wolfe","conditional-gradient","first-order","projection-free"]},{"location":"algorithm/frank-wolfe/#overview","title":"Overview","text":"<p>The Frank\u2013Wolfe (FW) algorithm, also called \"conditional gradient\", is an iterative first-order algorithm for solving constrained convex problems. The method was proposed by Marguerite Frank and Philip Wolfe in 1956.<sup>3</sup> Each update is a convex combination between the current iterate \\(\\mathsf{x^k}\\) and a minimimizer \\(\\mathsf{s^k}\\) of the linearization of \\(\\mathsf{f}\\) about \\(\\mathsf{x^k}\\) over the domain \\(\\mathcal{C}\\). This ensures each iteration is feasible. Several standard convergence properties hold for FW and its variants.</p>","tags":["algorithm","frank-wolfe","conditional-gradient","first-order","projection-free"]},{"location":"algorithm/frank-wolfe/#illustration","title":"Illustration","text":"<p>Consider the problem</p> \\[\\mathsf{\\min_{x \\in \\mathbb{R}^2} \\dfrac{1}{2} \\left\\|x-\\left[\\begin{array}{c} \\mathsf{6} \\\\ \\mathsf{1}\\end{array}\\right] \\right\\|^2 \\ \\ \\ \\mbox{s.t.}\\ \\ \\ \\left[\\begin{array}{rr} \\mathsf{2} &amp; \\mathsf{1} \\\\ \\mathsf{-4} &amp; \\mathsf{5} \\\\ \\mathsf{1} &amp; \\mathsf{-2}\\end{array}\\right]x \\leq \\left[\\begin{array}{c} \\mathsf{20} \\\\ \\mathsf{10} \\\\ \\mathsf{2} \\end{array}\\right],\\ \\ x\\geq 0.}\\] <p>Letting the set of feasible solutions be denoted by \\(\\mathcal{C}\\), below is an illustration of applying Frank-Wolfe with step-size \\(\\mathsf{\\alpha_k = 2 / (k + 2)}.\\)</p> <p></p>","tags":["algorithm","frank-wolfe","conditional-gradient","first-order","projection-free"]},{"location":"algorithm/frank-wolfe/#properties","title":"Properties","text":"<p>Convergence Theorem<sup>4</sup> </p> <p>If \\(\\mathsf{f}\\) is convex and Lipschitz differentiable over a convex and compact set \\(\\mathcal{C}\\), then there is a constant \\(\\mathsf{C&gt; 0}\\) such that</p> \\[ \\mathsf{f(x^k) \\leq f(x^\\star) + \\dfrac{C}{k+2},} \\] <p>where \\(\\mathsf{x^\\star}\\) is a minimizer of \\(\\mathsf{f}\\) over \\(\\mathcal{C}\\).</p> <p>Zigzag Behavior<sup>5</sup> </p> <p>Oscillations can occur...</p> <p>Invariant to Coordinate System<sup>6</sup></p> <p>Can rescale or re-jigger coordinates without any change in outcomes.</p> <p>Feasible + Projection-Free</p> <p>No projections needed if \\(\\mathsf{x^1 \\in \\mathcal{C}.}\\)</p>","tags":["algorithm","frank-wolfe","conditional-gradient","first-order","projection-free"]},{"location":"algorithm/frank-wolfe/#code","title":"Code","text":"Python (Linear Constraints)Python (Illustration) <pre><code>    from scipy.optimize import linprog\n    import numpy as np \n\n    def frankWolfe(grad, x_init, A_eq, b_eq, A_ineq, b_ineq,\n                   bnds, tol=1.0e-6):\n    ''' Minimize function subject to inequality constraints\n\n        Args:\n            grad:   function for gradient of cost \n            x_init: initial estimate\n            A_eq:   matrix for equality constraint\n            b_eq:   vector for equality constraint\n            A_ineq: matrix for inequality constraint\n            b_ineq: vector for inequality constraint  \n            bnds:   box constraint bounds   \n        Returns:\n            x: solution estimate\n    '''\n    k = 1.0  \n    x = x_init.copy() \n    converge = False\n    while not converge: \n        c   = grad(x).transpose()\n        opt = linprog(c=c, A_ub=A_ineq, b_ub=b_ineq,\n                      A_eq=A_eq, b_eq=b_eq, bounds=bnds)\n        s     = np.reshape(opt.x, x.shape)\n        alpha = 2.0 / (k + 2.0)\n        step  = alpha * (s - x)              \n        x    += step\n        k    += 1.0\n        converge = np.linalg.norm(step) &lt;= tol \n    return x   \n</code></pre> <pre><code>    lhs_ineq = [[ 2,  1], [-4,  5], [1, -2]]  \n    rhs_ineq = [20, 10, 2]\n    bnd      = [(0, float(\"inf\")), (0, float(\"inf\"))]\n    ref      = np.array([[6.0], [1.0]])\n    x_init   = np.array([[2.0], [2.0]])\n\n    def grad(x):\n        \"\"\" Compute gradient of 0.5 * || x - ref || ** 2\n        \"\"\"\n        return x - ref\n\n    sol = frankWolfe(grad, x_init, None, lhs_ineq, None,\n                     rhs_ineq, bnd)\n</code></pre>","tags":["algorithm","frank-wolfe","conditional-gradient","first-order","projection-free"]},{"location":"algorithm/frank-wolfe/#applications","title":"Applications","text":"<ul> <li>Statistics.</li> <li>Compressed sensing.</li> </ul>","tags":["algorithm","frank-wolfe","conditional-gradient","first-order","projection-free"]},{"location":"algorithm/frank-wolfe/#see-also","title":"See Also","text":"<ul> <li>Wikipedia: Frank-Wolfe algorithm</li> <li>Sparse Franke-Wolfe.</li> </ul> <ol> <li> <p>We let \\(\\mathbb{H}\\) be a real-valued finite dimensional Hilbert space (e.g \\(\\ \\mathbb{H} = \\mathbb{R}^{\\mathsf{n}}\\)).\u00a0\u21a9</p> </li> <li> <p>Here \\(\\overline{\\mathbb{R}}\\triangleq \\mathbb{R} \\cup \\infty.\\) \u21a9</p> </li> <li> <p>Frank, M., Wolfe, P. An algorithm for quadratic programming. Naval Research Logistics Quarterly. 1956.\u00a0\u21a9</p> </li> <li> <p>Jaggi, M. Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization. ICML. 2013.\u00a0\u21a9</p> </li> <li> <p>Insert citations to Jaggi paper.\u00a0\u21a9</p> </li> <li> <p>Use Taylor's theorem or something...\u00a0\u21a9</p> </li> </ol>","tags":["algorithm","frank-wolfe","conditional-gradient","first-order","projection-free"]},{"location":"model/lasso/","title":"Lasso","text":"","tags":["model","convex"]},{"location":"model/lasso/#model","title":"Model","text":"<p>For a matrix \\(\\mathsf{A \\in \\mathbb{R}^{m\\times n}}\\), a vector \\(\\mathsf{b \\in \\mathbb{R}^m}\\), and a scalar \\(\\mathsf{\\lambda &gt; 0}\\), the model is</p> \\[     \\mathsf{\\underset{x \\in \\mathbb{R}^n }{min} \\ |Ax - b|_2^2 + \\lambda |x|_1. } \\]","tags":["model","convex"]},{"location":"model/lasso/#overview","title":"Overview","text":"<p>The Least Absolute Shrinkage and Selection Operator (Lasso) function is commonly used in statistics and machine learning for variable selection and regularization. Introduced by Tibshirani<sup>1</sup>, it is primarily used in linear regression models, but its principles can be extended to other models as well. The key idea of the Lasso function is to add a penalty term to least squares regression that is the sum of absolute values of the coefficients.</p> <p></p> <p> Property Convex \u2705 Strongly Convex \ud83d\udfe1 Unbiased Estimator<sup>2</sup> \u2705 Feasible Estimator \ud83d\udfe1 <p></p> <p></p>","tags":["model","convex"]},{"location":"model/lasso/#code","title":"Code","text":"CVXPYSCIPGurobiCPLEX <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\n  std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre> <pre><code>#include &lt;stdio.h&gt;\n\nint main(void) {\n  printf(\"Hello world!\\n\");\n  return 0;\n}\n</code></pre> <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\n  std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre> <pre><code>#include &lt;iostream&gt;\n\nint main(void) {\n  std::cout &lt;&lt; \"Hello world!\" &lt;&lt; std::endl;\n  return 0;\n}\n</code></pre>","tags":["model","convex"]},{"location":"model/lasso/#applications","title":"Applications","text":"<ul> <li>Statistics... </li> <li>Compressed sensing...</li> </ul>","tags":["model","convex"]},{"location":"model/lasso/#see-also","title":"See Also","text":"<ul> <li>Wikipedia - Lasso (statistics)</li> </ul> <ol> <li> <p>Tibshirani. \"Regression shrinkage and selection via the lasso.\" Journal of the Royal Statistical Society Series B: Statistical Methodology. 1996\u00a0\u21a9</p> </li> <li> <p>Fan, Li. \"Variable selection via nonconcave penalized likelihood and its oracle properties.\" Journal of the American statistical Association. 2001.\u00a0\u21a9</p> </li> </ol>","tags":["model","convex"]},{"location":"tags/","title":"Tags","text":"<p>Below is a list of all tags on OptWiki.</p>"},{"location":"tags/#algorithm","title":"algorithm","text":"<ul> <li>Frank-Wolfe</li> </ul>"},{"location":"tags/#conditional-gradient","title":"conditional-gradient","text":"<ul> <li>Frank-Wolfe</li> </ul>"},{"location":"tags/#convex","title":"convex","text":"<ul> <li>Lasso</li> </ul>"},{"location":"tags/#first-order","title":"first-order","text":"<ul> <li>Frank-Wolfe</li> </ul>"},{"location":"tags/#frank-wolfe","title":"frank-wolfe","text":"<ul> <li>Frank-Wolfe</li> </ul>"},{"location":"tags/#model","title":"model","text":"<ul> <li>Lasso</li> </ul>"},{"location":"tags/#projection-free","title":"projection-free","text":"<ul> <li>Frank-Wolfe</li> </ul>"}]}